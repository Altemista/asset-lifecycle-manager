---
# Source: kubeapps/templates/mongodb-secret-bootstrap.yaml
apiVersion: v1
kind: Secret
metadata:
  name: kubeapps-mongodb
  annotations:
    helm.sh/hook: pre-install
  labels:
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
data:
  mongodb-root-password: "${MONGODB_PASSWORD}"
---
# Source: kubeapps/templates/dashboard-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubeapps-internal-dashboard-config
  labels:
    app: kubeapps-internal-dashboard-config
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
data:
  vhost.conf: |-
    server {
      listen 8080;
      server_name _;

      gzip on;
      gzip_static  on;

      location / {
        # Redirects are required to be relative otherwise the internal hostname will be exposed
        absolute_redirect off;

        # Trailing / is required in the path for the React app to be loaded correctly
        # The rewrite rule adds a trailing "/" to any path that does not contain "." neither "/".
        # i.e kubeapps => kubeapps/
        rewrite ^([^.]*[^/])$ $1/ permanent;

        # Support for ingress prefixes maintaining compatibility with the default /
        # 1 - Exactly two fragment URLs for files existing inside of the public/ dir
        # i.e /[prefix]/config.json => /config.json
        rewrite ^/[^/]+/([^/]+)$ /$1 break;

        # 2 - Any static files bundled by webpack referenced by 3 or more URL segments
        # i.e /[prefix]/static/main.js => static/main.js
        rewrite ^/[^/]+/static/(.*) /static/$1 break;

        try_files $uri /index.html;
      }
    }
  config.json: |-
    {
      "namespace": "altemistahub",
      "appVersion": "v1.7.2",
      "authProxyEnabled": false,
      "oauthLoginURI": "/oauth2/start",
      "oauthLogoutURI": "/oauth2/sign_out"
    }

---
# Source: kubeapps/templates/kubeapps-frontend-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubeapps-frontend-config
  labels:
    app: kubeapps-frontend-config
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
data:
  vhost.conf: |-
    # Retain the default nginx handling of requests without a "Connection" header
    map $http_upgrade $connection_upgrade {
      default upgrade;
      ''      close;
    }

    # Allow websocket connections
    proxy_set_header Upgrade    $http_upgrade;
    proxy_set_header Connection $connection_upgrade;

    server {
      listen 8080;
      server_name _;

      location /healthz {
        access_log off;
        default_type text/plain;
        return 200 "healthy\n";
      }

      # Using regexp match instead of prefix one because the application can be
      # deployed under a specific path i.e /kubeapps
      location ~* /api/kube {
        rewrite /api/kube/(.*) /$1 break;
        rewrite /api/kube / break;
        proxy_pass https://kubernetes.default;
        # Disable buffering for log streaming
        proxy_buffering off;
        # Hide Www-Authenticate to prevent it triggering a basic auth prompt in
        # the browser with some clusters
        proxy_hide_header Www-Authenticate;

        # Keep the connection open with the API server even if idle (the default is 60 seconds)
        # Setting it to 1 hour which should be enough for our current use case of deploying/upgrading apps
        # If we enable other use-cases in the future we might need to bump this value
        # More info here https://github.com/kubeapps/kubeapps/issues/766
        proxy_read_timeout 1h;
      }

      location ~* /api/chartsvc {
        rewrite /api/chartsvc/(.*) /chartsvc/$1 break;
        rewrite /api/chartsvc /chartsvc break;
        proxy_pass http://kubeapps-internal-tiller-proxy:8080;
      }

      location ~* /api/tiller-deploy {
        # Keep the connection open with the API server even if idle (the default is 60 seconds)
        # Setting it to 10 minutes which should be enough for our current use case of deploying/upgrading/deleting apps
        proxy_read_timeout 10m;
        rewrite /api/tiller-deploy/(.*) /$1 break;
        rewrite /api/tiller-deploy / break;
        proxy_pass http://kubeapps-internal-tiller-proxy:8080;
      }

      # The route for the Kubeapps backend API is not prefixed.
      location ~* /api/ {
        rewrite /api/(.*) /backend/$1 break;
        rewrite /api/ /backend break;

        proxy_pass http://kubeapps-internal-tiller-proxy:8080;
      }

      location / {
        # Add the Authorization header if exists
        add_header Authorization $http_authorization;

        proxy_pass http://kubeapps-internal-dashboard:8080;
      }
    }

---
# Source: kubeapps/templates/apprepository-jobs-cleanup-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubeapps-internal-apprepository-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    app: kubeapps-internal-apprepository-jobs-cleanup
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller

---
# Source: kubeapps/templates/apprepository-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    app: kubeapps-internal-apprepository-controller
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller

---
# Source: kubeapps/templates/mongodb-jobs-cleanup-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubeapps-internal-mongodb-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    app: kubeapps-internal-mongodb-jobs-cleanup
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller

---
# Source: kubeapps/templates/tiller-proxy-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubeapps-internal-tiller-proxy
  labels:
    app: kubeapps-internal-tiller-proxy
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller

---
# Source: kubeapps/templates/apprepository-crd.yaml
# The condition above will be true if another instance of Kubeapps is
# already installed
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: apprepositories.kubeapps.com
  annotations:
    "helm.sh/hook": crd-install
  labels:
    app: kubeapps-internal-apprepository-controller
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  group: kubeapps.com
  scope: Namespaced
  names:
    kind: AppRepository
    plural: apprepositories
    shortNames:
      - apprepos
  version: v1alpha1
---
# Source: kubeapps/templates/apprepository-jobs-cleanup-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: kubeapps-internal-apprepository-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    app: kubeapps-internal-apprepository-jobs-cleanup
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
rules:
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
    verbs:
      - list
      - delete
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - list
      - delete
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: kubeapps-internal-apprepository-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    app: kubeapps-internal-apprepository-jobs-cleanup
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubeapps-internal-apprepository-jobs-cleanup
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-apprepository-jobs-cleanup
    namespace: altemistahub
---
# Source: kubeapps/templates/apprepository-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    app: kubeapps-internal-apprepository-controller
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
  - apiGroups:
      - batch
    resources:
      - cronjobs
    verbs:
      - create
      - get
      - list
      - update
      - watch
      - delete
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - create
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
      - apprepositories/finalizers
    verbs:
      - get
      - list
      - update
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    app: kubeapps-internal-apprepository-controller
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubeapps-internal-apprepository-controller
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-apprepository-controller
    namespace: altemistahub
---
# Define role, but no binding, so users can be bound to this role
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: kubeapps-repositories-read
rules:
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
    verbs:
      - list
      - get
---
# Define role, but no binding, so users can be bound to this role
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: kubeapps-repositories-write
rules:
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
    verbs:
      - "*"
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
---
# Source: kubeapps/templates/mongodb-jobs-cleanup-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: kubeapps-internal-mongodb-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    app: kubeapps-internal-mongodb-jobs-cleanup
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - delete
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: kubeapps-internal-mongodb-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    app: kubeapps-internal-mongodb-jobs-cleanup
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubeapps-internal-mongodb-jobs-cleanup
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-mongodb-jobs-cleanup
    namespace: altemistahub
---
# Source: kubeapps/templates/tiller-proxy-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: kubeapps-internal-tiller-proxy
  labels:
    app: kubeapps-internal-tiller-proxy
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
  - apiGroups:
      - "kubeapps.com"
    resources:
      - apprepositories
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: kubeapps-internal-tiller-proxy
  labels:
    app: kubeapps-internal-tiller-proxy
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubeapps-internal-tiller-proxy
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-tiller-proxy
    namespace: altemistahub

---
# Source: kubeapps/charts/mongodb/templates/svc-standalone.yaml

apiVersion: v1
kind: Service
metadata:
  name: kubeapps-mongodb
  labels:
    app: mongodb
    chart: mongodb-7.2.9
    release: "kubeapps"
    heritage: "Tiller"
spec:
  type: ClusterIP
  ports:
  - name: mongodb
    port: 27017
    targetPort: mongodb
  selector:
    app: mongodb
    release: "kubeapps"

---
# Source: kubeapps/templates/chartsvc-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-internal-chartsvc
  labels:
    app: kubeapps
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps-internal-chartsvc
    release: kubeapps

---
# Source: kubeapps/templates/dashboard-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-internal-dashboard
  labels:
    app: kubeapps
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps-internal-dashboard
    release: kubeapps

---
# Source: kubeapps/templates/kubeapps-frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps
  labels:
    app: kubeapps
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps
    release: kubeapps

---
# Source: kubeapps/templates/tiller-proxy-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-internal-tiller-proxy
  labels:
    app: kubeapps
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps-internal-tiller-proxy
    release: kubeapps

---
# Source: kubeapps/templates/tests/test-chartsvc.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "kubeapps-chartsvc-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: kubeapps-chartsvc-test
      image: bitnami/nginx:1.16.1-debian-9-r52
      env:
        - name: CHARTSVC_HOST
          value: kubeapps-internal-chartsvc.altemistahub
        - name: CHARTSVC_PORT
          value: "8080"
      command:
        - sh
        - -c
        - curl -o /tmp/output $CHARTSVC_HOST:$CHARTSVC_PORT/v1/charts && cat /tmp/output && cat /tmp/output | grep wordpress
  restartPolicy: Never

---
# Source: kubeapps/templates/tests/test-dashboard.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "kubeapps-dashboard-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: kubeapps-dashboard-test
      image: bitnami/nginx:1.16.1-debian-9-r52
      env:
        - name: DASHBOARD_HOST
          value: kubeapps.altemistahub
      command:
        - sh
        - -c
        - curl -o /tmp/output $DASHBOARD_HOST && cat /tmp/output && cat /tmp/output | grep 'You need to enable JavaScript to run this app'
  restartPolicy: Never

---
# Source: kubeapps/templates/tests/test-tiller-proxy.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "kubeapps-tiller-proxy-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: kubeapps-tiller-proxy-test
      image: bitnami/nginx:1.16.1-debian-9-r52
      env:
        - name: TILLER_PROXY_HOST
          value: kubeapps-internal-tiller-proxy.altemistahub
        - name: TILLER_PROXY_PORT
          value: "8080"
        - name: KUBEAPPS_RELEASE
          value: kubeapps
      command:
        - sh
        - -c
        - "curl -o /tmp/output -ik -H \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" $TILLER_PROXY_HOST:$TILLER_PROXY_PORT/v1/releases && cat /tmp/output && cat /tmp/output | grep $KUBEAPPS_RELEASE"
  restartPolicy: Never

---
# Source: kubeapps/charts/mongodb/templates/deployment-standalone.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-mongodb
  labels:
    app: mongodb
    chart: mongodb-7.2.9
    release: "kubeapps"
    heritage: "Tiller"
spec:
  selector:
    matchLabels:
      app: mongodb
      release: "kubeapps"
  template:
    metadata:
      labels:
        app: mongodb
        release: "kubeapps"
        chart: mongodb-7.2.9
    spec:
      initContainers:
      containers:
      - name: kubeapps-mongodb
        image: docker.io/bitnami/mongodb:4.0.12-debian-9-r43
        imagePullPolicy: "IfNotPresent"
        env:
        - name: MONGODB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: kubeapps-mongodb
              key: mongodb-root-password
        - name: MONGODB_SYSTEM_LOG_VERBOSITY
          value: "0"
        - name: MONGODB_DISABLE_SYSTEM_LOG
          value: "no"
        - name: MONGODB_ENABLE_IPV6
          value: "no"
        - name: MONGODB_ENABLE_DIRECTORY_PER_DB
          value: "no"
        ports:
        - name: mongodb
          containerPort: 27017
        livenessProbe:
          exec:
            command:
            - mongo
            - --eval
            - "db.adminCommand('ping')"
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 6
        readinessProbe:
          exec:
            command:
            - mongo
            - --eval
            - "db.adminCommand('ping')"
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 6
        volumeMounts:
        - name: data
          mountPath: /bitnami/mongodb
          subPath:
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 50m
            memory: 256Mi

      volumes:
      - name: data
        emptyDir: {}
---
# Source: kubeapps/templates/apprepository-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    app: kubeapps-internal-apprepository-controller
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kubeapps-internal-apprepository-controller
      release: kubeapps
  template:
    metadata:
      labels:
        app: kubeapps-internal-apprepository-controller
        release: kubeapps
    spec:
      serviceAccountName: kubeapps-internal-apprepository-controller
      securityContext:
        fsGroup:
        runAsUser: 1001
      containers:
        - name: controller
          image: docker.io/bitnami/kubeapps-apprepository-controller:1.7.2-scratch-r0
          command:
            - /apprepository-controller
          args:
            - --logtostderr
            - --user-agent-comment=kubeapps/v1.7.2
            - --repo-sync-image=docker.io/bitnami/kubeapps-chart-repo:1.9.0-r2
            - --namespace=altemistahub
            - --mongo-url=kubeapps-mongodb
            - --mongo-secret-name=kubeapps-mongodb
          resources:
            limits:
              cpu: 250m
              memory: 128Mi
            requests:
              cpu: 25m
              memory: 32Mi


---
# Source: kubeapps/templates/chartsvc-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-chartsvc
  labels:
    app: kubeapps-internal-chartsvc
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps-internal-chartsvc
      release: kubeapps
  template:
    metadata:
      labels:
        app: kubeapps-internal-chartsvc
        release: kubeapps
    spec:
      securityContext:
        fsGroup:
        runAsUser: 1001
      containers:
        - name: chartsvc
          image: docker.io/bitnami/kubeapps-chartsvc:1.9.0-r0
          command:
            - /chartsvc
          args:
            - --mongo-user=root
            - --mongo-url=kubeapps-mongodb
          env:
            - name: MONGO_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: kubeapps-mongodb
                  key: mongodb-root-password
          ports:
            - name: http
              containerPort: 8080
          livenessProbe:
            httpGet:
              path: /live
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5

          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5


---
# Source: kubeapps/templates/dashboard-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-dashboard
  labels:
    app: kubeapps-internal-dashboard
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps-internal-dashboard
      release: kubeapps
  template:
    metadata:
      annotations:
        checksum/config: a4401ee0447c39425b8262ae267575dd57f56e0d12bc41d2ff3f8b510a4092ee
      labels:
        app: kubeapps-internal-dashboard
        release: kubeapps
    spec:
      securityContext:
        fsGroup:
        runAsUser: 1001
      containers:
        - name: dashboard
          image: docker.io/bitnami/kubeapps-dashboard:1.7.2-debian-9-r0
          livenessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5

          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5

          volumeMounts:
            - name: vhost
              mountPath: /opt/bitnami/nginx/conf/server_blocks
            - name: config
              mountPath: /app/config.json
              subPath: config.json
          ports:
            - name: http
              containerPort: 8080
          resources:
            limits:
              cpu: 250m
              memory: 128Mi
            requests:
              cpu: 25m
              memory: 32Mi

      volumes:
        - name: vhost
          configMap:
            name: kubeapps-internal-dashboard-config
            items:
              - key: vhost.conf
                path: vhost.conf
        - name: config
          configMap:
            name: kubeapps-internal-dashboard-config
            items:
              - key: config.json
                path: config.json

---
# Source: kubeapps/templates/kubeapps-frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps
  labels:
    app: kubeapps
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps
      release: kubeapps
  template:
    metadata:
      annotations:
        checksum/config: b3fdde2dc938a5ee973888d20710a4e926731c0990cff3ca475f3f32fbfa421c
      labels:
        app: kubeapps
        release: kubeapps
    spec:
      securityContext:
        fsGroup:
        runAsUser: 1001
      containers:
        - name: nginx
          image: docker.io/bitnami/nginx:1.16.1-debian-9-r52
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5

          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5

          volumeMounts:
            - name: vhost
              mountPath: /opt/bitnami/nginx/conf/server_blocks
          ports:
            - name: http
              containerPort: 8080
          resources:
            limits:
              cpu: 250m
              memory: 128Mi
            requests:
              cpu: 25m
              memory: 32Mi

      volumes:
        - name: vhost
          configMap:
            name: kubeapps-frontend-config

---
# Source: kubeapps/templates/tiller-proxy-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-tiller-proxy
  labels:
    app: kubeapps-internal-tiller-proxy
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps-internal-tiller-proxy
      release: kubeapps
  template:
    metadata:
      labels:
        app: kubeapps-internal-tiller-proxy
        release: kubeapps
    spec:
      serviceAccountName: kubeapps-internal-tiller-proxy
      # Increase termination timeout to let remaining operations to finish before killing the pods
      # This is because new releases/upgrades/deletions are synchronous operations
      terminationGracePeriodSeconds: 300
      securityContext:
        fsGroup:
        runAsUser: 1001
      containers:
        - name: proxy
          image: docker.io/bitnami/kubeapps-tiller-proxy:1.7.2-scratch-r0
          command:
            - /proxy
          args:
            - --host=tiller-deploy.kube-system:44134
            - --user-agent-comment=kubeapps/v1.7.2
            - --chartsvc-url=http://kubeapps-internal-chartsvc:8080
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 8080
          livenessProbe:
            httpGet:
              path: /live
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5

          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5

          resources:
            limits:
              cpu: 250m
              memory: 256Mi
            requests:
              cpu: 25m
              memory: 32Mi


---
# Source: kubeapps/templates/apprepository-jobs-cleanup.yaml
# Clean up the AppRepository resources used by this Kubeapps instance
apiVersion: batch/v1
kind: Job
metadata:
  name: kubeapps-internal-apprepository-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
  labels:
    app: kubeapps-internal-apprepository-jobs-cleanup
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  template:
    metadata:
      labels:
        app: kubeapps-internal-apprepository-jobs-cleanup
        release: kubeapps
    spec:
      securityContext:
        fsGroup:
        runAsUser: 1001
      restartPolicy: OnFailure
      serviceAccountName: kubeapps-internal-apprepository-jobs-cleanup
      containers:
        - name: kubectl
          image: docker.io/bitnami/kubectl:1.16.3-r17
          command:
            - /bin/sh
          args:
            - -ec
            - |
              kubectl delete apprepositories.kubeapps.com -n altemistahub --all
              kubectl delete secrets -n altemistahub -l app=kubeapps,release=kubeapps

---
# Source: kubeapps/templates/mongodb-jobs-cleanup.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: kubeapps-internal-mongodb-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
  labels:
    app: kubeapps-internal-mongodb-jobs-cleanup
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  template:
    metadata:
      labels:
        app: kubeapps-internal-mongodb-jobs-cleanup
        release: kubeapps
    spec:
      securityContext:
        fsGroup:
        runAsUser: 1001
      restartPolicy: OnFailure
      serviceAccountName: kubeapps-internal-mongodb-jobs-cleanup
      containers:
        - name: kubectl
          image: docker.io/bitnami/kubectl:1.16.3-r17
          command:
            - /bin/sh
          args:
            - -c
            - "kubectl delete secret -n altemistahub kubeapps-mongodb || true"

---
# Source: kubeapps/templates/ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubeapps
  labels:
    app: kubeapps
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-staging"
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
spec:
  rules:
    - host: ${KUBEAPPS_HOSTNAME}
      http:
        paths:
        - path: /
          backend:
            serviceName: kubeapps
            servicePort: http
    ## The block below is deprecated and must removed on 3.0.0
    - host: kubeapps.local
      http:
        paths:
        - path: /
          backend:
            serviceName: kubeapps
            servicePort: http
    ## end of block
  tls:
    ## The block below is deprecated and must removed on 3.0.0
    ## end of block
    - hosts:
        - ${KUBEAPPS_HOSTNAME}
      secretName: ${KUBEAPPS_HOSTNAME}-tls

---
# Source: kubeapps/templates/apprepositories.yaml

apiVersion: kubeapps.com/v1alpha1
kind: AppRepository
metadata:
  name: altemista
  annotations:
    "helm.sh/hook": pre-install
  labels:
    app: kubeapps-internal-apprepository-controller
    chart: kubeapps-3.2.1
    release: kubeapps
    heritage: Tiller
spec:
  type: helm
  url: https://core.harbor.apps.altemista.35.242.157.252.nip.io/chartrepo/library
  syncJobPodTemplate:
    spec:
      securityContext:
        runAsUser: 1001
---

---
# Source: kubeapps/charts/mongodb/templates/configmap.yaml

---
# Source: kubeapps/charts/mongodb/templates/headless-svc-rs.yaml


---
# Source: kubeapps/charts/mongodb/templates/ingress.yaml

---
# Source: kubeapps/charts/mongodb/templates/initialization-configmap.yaml


---
# Source: kubeapps/charts/mongodb/templates/poddisruptionbudget-arbiter-rs.yaml

---
# Source: kubeapps/charts/mongodb/templates/poddisruptionbudget-secondary-rs.yaml


---
# Source: kubeapps/charts/mongodb/templates/prometheus-alerting-rule.yaml


---
# Source: kubeapps/charts/mongodb/templates/prometheus-service-monitor.yaml


---
# Source: kubeapps/charts/mongodb/templates/pvc-standalone.yaml


---
# Source: kubeapps/charts/mongodb/templates/secrets.yaml


---
# Source: kubeapps/charts/mongodb/templates/statefulset-arbiter-rs.yaml


---
# Source: kubeapps/charts/mongodb/templates/statefulset-primary-rs.yaml


---
# Source: kubeapps/charts/mongodb/templates/statefulset-secondary-rs.yaml


---
# Source: kubeapps/charts/mongodb/templates/svc-primary-rs.yaml


---
# Source: kubeapps/templates/apprepositories-secret.yaml


---
# Source: kubeapps/templates/kubeops-deployment.yaml


---
# Source: kubeapps/templates/kubeops-service.yaml


---
# Source: kubeapps/templates/kubeops-serviceaccount.yaml


---
# Source: kubeapps/templates/tiller-proxy-secret.yaml
# The tls ca certificate is only required when tls.verify is set to true, we fail otherwise.

---
# Source: kubeapps/templates/tls-secrets.yaml


