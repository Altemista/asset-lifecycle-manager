---
# Source: kubeapps/templates/ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubeapps
  labels: 
    app: kubeapps
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
  annotations:
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
spec:
  rules:
    - host: ${KUBEAPPS_HOSTNAME}
      http:
        paths:
        - path: /
          backend:
            serviceName: kubeapps
            servicePort: http
    ## The block below is deprecated and must removed on 3.0.0
    ## end of block

---
# Source: kubeapps/templates/kubeapps-frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps
  labels:
    app: kubeapps
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps
      release: kubeapps
  template:
    metadata:
      annotations:
        checksum/config: a21e59e22c7d96e337df9a54cd0dd71a8c0c860d2625364e0691cc215f4d9275
      labels:
        app: kubeapps
        release: kubeapps
    spec:      
      securityContext:
        fsGroup: 
        runAsUser: 1001
      containers:
        - name: nginx
          image: docker.io/bitnami/nginx:1.16.1-debian-9-r52
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5
            
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5
            
          volumeMounts:
            - name: vhost
              mountPath: /opt/bitnami/nginx/conf/server_blocks
          ports:
            - name: http
              containerPort: 8080
          resources:
            limits:
              cpu: 250m
              memory: 128Mi
            requests:
              cpu: 25m
              memory: 32Mi
            
      volumes:
        - name: vhost
          configMap:
            name: kubeapps-frontend-config

---
# Source: kubeapps/templates/tiller-proxy-secret.yaml
# The tls ca certificate is only required when tls.verify is set to true, we fail otherwise.

---
# Source: kubeapps/templates/apprepository-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    app: kubeapps-internal-apprepository-controller
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
  - apiGroups:
      - batch
    resources:
      - cronjobs
    verbs:
      - create
      - get
      - list
      - update
      - watch
      - delete
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - create
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
      - apprepositories/finalizers
    verbs:
      - get
      - list
      - update
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    app: kubeapps-internal-apprepository-controller
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubeapps-internal-apprepository-controller
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-apprepository-controller
    namespace: altemistahub
---
# Define role, but no binding, so users can be bound to this role
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: kubeapps-repositories-read
rules:
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
    verbs:
      - list
      - get
---
# Define role, but no binding, so users can be bound to this role
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: kubeapps-repositories-write
rules:
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
    verbs:
      - "*"
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
---
# Source: kubeapps/templates/assetsvc-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-assetsvc
  labels:
    app: kubeapps-internal-assetsvc
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps-internal-assetsvc
      release: kubeapps
  template:
    metadata:
      labels:
        app: kubeapps-internal-assetsvc
        release: kubeapps
    spec:      
      securityContext:
        fsGroup: 
        runAsUser: 1001
      containers:
        - name: assetsvc
          image: docker.io/bitnami/kubeapps-assetsvc:1.8.1-scratch-r0
          command:
            - /assetsvc
          args:
            - --database-type=mongodb
            - --database-user=root
            - --database-name=charts
            - --database-url=kubeapps-mongodb
          env:
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: kubeapps-mongodb
                  key: mongodb-root-password
          ports:
            - name: http
              containerPort: 8080
          livenessProbe:
            httpGet:
              path: /live
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5
            
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5
            

---
# Source: kubeapps/templates/kubeapps-frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps
  labels:
    app: kubeapps
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps
    release: kubeapps

---
# Source: kubeapps/templates/apprepository-crd.yaml
# The condition above will be true if another instance of Kubeapps is
# already installed
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: apprepositories.kubeapps.com
  annotations:
    "helm.sh/hook": crd-install
  labels:
    app: kubeapps-internal-apprepository-controller
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
spec:
  group: kubeapps.com
  scope: Namespaced
  names:
    kind: AppRepository
    plural: apprepositories
    shortNames:
      - apprepos
  version: v1alpha1
---
# Source: kubeapps/templates/tiller-proxy-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubeapps-internal-tiller-proxy
  labels:
    app: kubeapps-internal-tiller-proxy
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller

---
# Source: kubeapps/templates/tiller-proxy-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-internal-tiller-proxy
  labels:
    app: kubeapps
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps-internal-tiller-proxy
    release: kubeapps

---
# Source: kubeapps/templates/dashboard-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-internal-dashboard
  labels:
    app: kubeapps
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps-internal-dashboard
    release: kubeapps

---
# Source: kubeapps/templates/apprepository-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    app: kubeapps-internal-apprepository-controller
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller

---
# Source: kubeapps/templates/db-secret-bootstrap.yaml
apiVersion: v1
kind: Secret
metadata:
  name: kubeapps-mongodb
  annotations:
    helm.sh/hook: pre-install
  labels:
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
data:
  mongodb-root-password: "M3NOeWRhSHdCcQ=="
---
# Source: kubeapps/templates/kubeapps-frontend-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubeapps-frontend-config
  labels:
    app: kubeapps-frontend-config
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
data:
  vhost.conf: |-
    # Retain the default nginx handling of requests without a "Connection" header
    map $http_upgrade $connection_upgrade {
      default upgrade;
      ''      close;
    }

    # Allow websocket connections
    proxy_set_header Upgrade    $http_upgrade;
    proxy_set_header Connection $connection_upgrade;

    server {
      listen 8080;
      server_name _;

      location /healthz {
        access_log off;
        default_type text/plain;
        return 200 "healthy\n";
      }

      # Using regexp match instead of prefix one because the application can be
      # deployed under a specific path i.e /kubeapps
      location ~* /api/kube {
        rewrite /api/kube/(.*) /$1 break;
        rewrite /api/kube / break;
        proxy_pass https://kubernetes.default;
        # Disable buffering for log streaming
        proxy_buffering off;
        # Hide Www-Authenticate to prevent it triggering a basic auth prompt in
        # the browser with some clusters
        proxy_hide_header Www-Authenticate;

        # Keep the connection open with the API server even if idle (the default is 60 seconds)
        # Setting it to 1 hour which should be enough for our current use case of deploying/upgrading apps
        # If we enable other use-cases in the future we might need to bump this value
        # More info here https://github.com/kubeapps/kubeapps/issues/766
        proxy_read_timeout 1h;
      }

      location ~* /api/assetsvc {
        rewrite /api/assetsvc/(.*) /assetsvc/$1 break;
        rewrite /api/assetsvc /assetsvc break;

        proxy_pass http://kubeapps-internal-tiller-proxy:8080;
      }

      location ~* /api/tiller-deploy {
        # Keep the connection open with the API server even if idle (the default is 60 seconds)
        # Setting it to 10 minutes which should be enough for our current use case of deploying/upgrading/deleting apps
        proxy_read_timeout 10m;
        rewrite /api/tiller-deploy/(.*) /$1 break;
        rewrite /api/tiller-deploy / break;
        proxy_pass http://kubeapps-internal-tiller-proxy:8080;
      }

      # The route for the Kubeapps backend API is not prefixed.
      location ~* /api/ {
        rewrite /api/(.*) /backend/$1 break;
        rewrite /api/ /backend break;

        proxy_pass http://kubeapps-internal-tiller-proxy:8080;
      }

      location / {
        # Add the Authorization header if exists
        add_header Authorization $http_authorization;

        proxy_pass http://kubeapps-internal-dashboard:8080;
      }
    }

---
# Source: kubeapps/templates/tiller-proxy-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: kubeapps-internal-tiller-proxy
  labels:
    app: kubeapps-internal-tiller-proxy
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
  - apiGroups:
      - "kubeapps.com"
    resources:
      - apprepositories
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: kubeapps-internal-tiller-proxy
  labels:
    app: kubeapps-internal-tiller-proxy
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubeapps-internal-tiller-proxy
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-tiller-proxy
    namespace: altemistahub

---
# Source: kubeapps/templates/tiller-proxy-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-tiller-proxy
  labels:
    app: kubeapps-internal-tiller-proxy
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps-internal-tiller-proxy
      release: kubeapps
  template:
    metadata:
      labels:
        app: kubeapps-internal-tiller-proxy
        release: kubeapps
    spec:
      serviceAccountName: kubeapps-internal-tiller-proxy
      # Increase termination timeout to let remaining operations to finish before killing the pods
      # This is because new releases/upgrades/deletions are synchronous operations
      terminationGracePeriodSeconds: 300      
      securityContext:
        fsGroup: 
        runAsUser: 1001
      containers:
        - name: proxy
          image: docker.io/bitnami/kubeapps-tiller-proxy:1.8.1-scratch-r0
          command:
            - /proxy
          args:
            - --host=tiller-deploy.kube-system:44134
            - --user-agent-comment=kubeapps/v1.8.1
            - --assetsvc-url=http://kubeapps-internal-assetsvc:8080
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 8080
          livenessProbe:
            httpGet:
              path: /live
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5
            
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5
            
          resources:
            limits:
              cpu: 250m
              memory: 256Mi
            requests:
              cpu: 25m
              memory: 32Mi
            

---
# Source: kubeapps/templates/dashboard-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-dashboard
  labels:
    app: kubeapps-internal-dashboard
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps-internal-dashboard
      release: kubeapps
  template:
    metadata:
      annotations:
        checksum/config: 5a3c88daeb118c8425cf7d03327785d2170312d30b60b654162e56f56260baf1
      labels:
        app: kubeapps-internal-dashboard
        release: kubeapps
    spec:      
      securityContext:
        fsGroup: 
        runAsUser: 1001
      containers:
        - name: dashboard
          image: docker.io/bitnami/kubeapps-dashboard:1.8.1-debian-10-r0
          livenessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5
            
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5
            
          volumeMounts:
            - name: vhost
              mountPath: /opt/bitnami/nginx/conf/server_blocks
            - name: config
              mountPath: /app/config.json
              subPath: config.json
          ports:
            - name: http
              containerPort: 8080
          resources:
            limits:
              cpu: 250m
              memory: 128Mi
            requests:
              cpu: 25m
              memory: 32Mi
            
      volumes:
        - name: vhost
          configMap:
            name: kubeapps-internal-dashboard-config
            items:
              - key: vhost.conf
                path: vhost.conf
        - name: config
          configMap:
            name: kubeapps-internal-dashboard-config
            items:
              - key: config.json
                path: config.json

---
# Source: kubeapps/templates/apprepository-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    app: kubeapps-internal-apprepository-controller
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kubeapps-internal-apprepository-controller
      release: kubeapps
  template:
    metadata:
      labels:
        app: kubeapps-internal-apprepository-controller
        release: kubeapps
    spec:
      serviceAccountName: kubeapps-internal-apprepository-controller      
      securityContext:
        fsGroup: 
        runAsUser: 1001
      containers:
        - name: controller
          image: docker.io/bitnami/kubeapps-apprepository-controller:1.8.1-scratch-r0
          command:
            - /apprepository-controller
          args:
            - --user-agent-comment=kubeapps/v1.8.1
            - --repo-sync-image=docker.io/bitnami/kubeapps-asset-syncer:1.8.1-scratch-r0
            - --repo-sync-cmd=/asset-syncer
            - --namespace=altemistahub
            - --database-secret-name=kubeapps-mongodb
            - --database-secret-key=mongodb-root-password
            - --database-type=mongodb
            - --database-url=kubeapps-mongodb
            - --database-user=root
            - --database-name=charts
          resources:
            limits:
              cpu: 250m
              memory: 128Mi
            requests:
              cpu: 25m
              memory: 32Mi
            

---
# Source: kubeapps/templates/dashboard-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubeapps-internal-dashboard-config
  labels:
    app: kubeapps-internal-dashboard-config
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
data:
  vhost.conf: |-
    server {
      listen 8080;
      server_name _;

      gzip on;
      gzip_static  on;

      location / {
        # Redirects are required to be relative otherwise the internal hostname will be exposed
        absolute_redirect off;

        # Trailing / is required in the path for the React app to be loaded correctly
        # The rewrite rule adds a trailing "/" to any path that does not contain "." neither "/".
        # i.e kubeapps => kubeapps/
        rewrite ^([^.]*[^/])$ $1/ permanent;

        # Support for ingress prefixes maintaining compatibility with the default /
        # 1 - Exactly two fragment URLs for files existing inside of the public/ dir
        # i.e /[prefix]/config.json => /config.json
        rewrite ^/[^/]+/([^/]+)$ /$1 break;

        # 2 - Any static files bundled by webpack referenced by 3 or more URL segments
        # i.e /[prefix]/static/main.js => static/main.js
        rewrite ^/[^/]+/static/(.*) /static/$1 break;

        try_files $uri /index.html;
      }
    }
  config.json: |-
    {
      "namespace": "altemistahub",
      "appVersion": "v1.8.1",
      "authProxyEnabled": false,
      "oauthLoginURI": "/oauth2/start",
      "oauthLogoutURI": "/oauth2/sign_out"
    }

---
# Source: kubeapps/templates/assetsvc-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-internal-assetsvc
  labels:
    app: kubeapps
    chart: kubeapps-3.3.1
    release: kubeapps
    heritage: Tiller
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps-internal-assetsvc
    release: kubeapps

---
# Source: kubeapps/charts/mongodb/templates/svc-standalone.yaml

apiVersion: v1
kind: Service
metadata:
  name: kubeapps-mongodb
  labels:
    app: mongodb
    chart: mongodb-7.6.1
    release: "kubeapps"
    heritage: "Tiller"
spec:
  type: ClusterIP
  ports:
  - name: mongodb
    port: 27017
    targetPort: mongodb
  selector:
    app: mongodb
    release: "kubeapps"

---
# Source: kubeapps/charts/mongodb/templates/deployment-standalone.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-mongodb
  labels:
    app: mongodb
    chart: mongodb-7.6.1
    release: "kubeapps"
    heritage: "Tiller"
spec:
  selector:
    matchLabels:
      app: mongodb
      release: "kubeapps"
  template:
    metadata:
      labels:
        app: mongodb
        release: "kubeapps"
        chart: mongodb-7.6.1
    spec:      
      initContainers:
      containers:
      - name: kubeapps-mongodb
        image: docker.io/bitnami/mongodb:4.0.14-debian-9-r0
        imagePullPolicy: "IfNotPresent"
        env:
        - name: MONGODB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: kubeapps-mongodb
              key: mongodb-root-password
        - name: MONGODB_SYSTEM_LOG_VERBOSITY
          value: "0"
        - name: MONGODB_DISABLE_SYSTEM_LOG
          value: "no"
        - name: MONGODB_ENABLE_IPV6
          value: "no"
        - name: MONGODB_ENABLE_DIRECTORY_PER_DB
          value: "no"
        ports:
        - name: mongodb
          containerPort: 27017
        livenessProbe:
          exec:
            command:
            - mongo
            - --eval
            - "db.adminCommand('ping')"
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 6
        readinessProbe:
          exec:
            command:
            - mongo
            - --eval
            - "db.adminCommand('ping')"
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 6
        volumeMounts:
        - name: data
          mountPath: /bitnami/mongodb
          subPath: 
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 50m
            memory: 256Mi
          
      volumes:
      - name: data
        emptyDir: {}
